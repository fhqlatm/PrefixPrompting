{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fce07719",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leej/anaconda3/envs/kcc/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Union, List, Dict, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig, MODEL_FOR_MASKED_LM_MAPPING\n",
    "from transformers.models.roberta.modeling_roberta import RobertaPreTrainedModel, RobertaModel, RobertaLMHead\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput, BaseModelOutputWithPoolingAndCrossAttentions\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c53c3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_data = '../data'\n",
    "prepro_train = 'train.json'\n",
    "prepro_test = 'test.json'\n",
    "PATH_train = os.path.join(PATH_data, prepro_train)\n",
    "PATH_test = os.path.join(PATH_data, prepro_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecd64c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETAINED_MODEL_NAME = 'klue/roberta-base'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRETAINED_MODEL_NAME)\n",
    "config = AutoConfig.from_pretrained(PRETAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f98184ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CONFIG_CLASSES = list(MODEL_FOR_MASKED_LM_MAPPING.keys())\n",
    "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n",
    "    \"\"\"\n",
    "\n",
    "    # Huggingface's original arguments\n",
    "    model_name_or_path: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"The model checkpoint for weights initialization.\"\n",
    "            \"Don't set if you want to train a model from scratch.\"\n",
    "        },\n",
    "    )\n",
    "    model_type: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"If training from scratch, pass a model type from the list: \" + \", \".join(MODEL_TYPES)},\n",
    "    )\n",
    "    config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
    "    )\n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
    "    )\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n",
    "    )\n",
    "    use_fast_tokenizer: bool = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n",
    "    )\n",
    "    model_revision: str = field(\n",
    "        default=\"main\",\n",
    "        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n",
    "    )\n",
    "    use_auth_token: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": \"Will use the token generated when running `transformers-cli login` (necessary to use this script \"\n",
    "            \"with private models).\"\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # SimCSE's arguments\n",
    "    temp: float = field(\n",
    "        default=0.05,\n",
    "        metadata={\n",
    "            \"help\": \"Temperature for softmax.\"\n",
    "        }\n",
    "    )\n",
    "    pooler_type: str = field(\n",
    "        default=\"cls\",\n",
    "        metadata={\n",
    "            \"help\": \"What kind of pooler to use (cls, cls_before_pooler, avg, avg_top2, avg_first_last).\"\n",
    "        }\n",
    "    ) \n",
    "    hard_negative_weight: float = field(\n",
    "        default=0,\n",
    "        metadata={\n",
    "            \"help\": \"The **logit** of weight for hard negatives (only effective if hard negatives are used).\"\n",
    "        }\n",
    "    )\n",
    "    do_mlm: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": \"Whether to use MLM auxiliary objective.\"\n",
    "        }\n",
    "    )\n",
    "    mlm_weight: float = field(\n",
    "        default=0.1,\n",
    "        metadata={\n",
    "            \"help\": \"Weight for MLM auxiliary objective (only effective if --do_mlm).\"\n",
    "        }\n",
    "    )\n",
    "    mlp_only_train: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": \"Use MLP only during training\"\n",
    "        }\n",
    "    )\n",
    "    pre_seq_len: int = field(\n",
    "        default=10,\n",
    "        metadata={\n",
    "            \"help\": \"The length of prompt\"\n",
    "        }\n",
    "    )\n",
    "    prefix_projection: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": \"Apply a two-layer MLP head over the prefix embeddings\"\n",
    "        }\n",
    "    ) \n",
    "    prefix_hidden_size: int = field(\n",
    "        default=512,\n",
    "        metadata={\n",
    "            \"help\": \"The hidden size of the MLP projection head in Prefix Encoder if prefix projection is used\"\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3d2d291",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrefixEncoder(torch.nn.Module):\n",
    "    r'''\n",
    "    The torch.nn model to encode the prefix\n",
    "    Input shape: (batch-size, prefix-length)\n",
    "    Output shape: (batch-size, prefix-length, 2*layers*hidden)\n",
    "    '''\n",
    "    def __init__(self, config, model_args):\n",
    "        super().__init__()\n",
    "        self.prefix_projection = model_args.prefix_projection\n",
    "        if self.prefix_projection:\n",
    "            # Use a two-layer MLP to encode the prefix\n",
    "            self.embedding = torch.nn.Embedding(model_args.pre_seq_len, config.hidden_size)\n",
    "            self.trans = torch.nn.Sequential(\n",
    "                torch.nn.Linear(config.hidden_size, model_args.prefix_hidden_size),\n",
    "                torch.nn.Tanh(),\n",
    "                torch.nn.Linear(model_args.prefix_hidden_size, config.num_hidden_layers * 2 * config.hidden_size)\n",
    "            )\n",
    "        else:\n",
    "            self.embedding = torch.nn.Embedding(model_args.pre_seq_len, config.num_hidden_layers * 2 * config.hidden_size)\n",
    "\n",
    "    def forward(self, prefix: torch.Tensor):\n",
    "        if self.prefix_projection:\n",
    "            prefix_tokens = self.embedding(prefix)\n",
    "            past_key_values = self.trans(prefix_tokens)\n",
    "        else:\n",
    "            past_key_values = self.embedding(prefix)\n",
    "        return past_key_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce221f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Head for getting sentence representations over RoBERTa/BERT's CLS representation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, features, **kwargs):\n",
    "        x = self.dense(features)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Similarity(nn.Module):\n",
    "    \"\"\"\n",
    "    Dot product or cosine similarity\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, temp):\n",
    "        super().__init__()\n",
    "        self.temp = temp\n",
    "        self.cos = nn.CosineSimilarity(dim=-1)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        return self.cos(x, y) / self.temp\n",
    "\n",
    "\n",
    "class Pooler(nn.Module):\n",
    "    \"\"\"\n",
    "    Parameter-free poolers to get the sentence embedding\n",
    "    'cls': [CLS] representation with BERT/RoBERTa's MLP pooler.\n",
    "    'cls_before_pooler': [CLS] representation without the original MLP pooler.\n",
    "    'avg': average of the last layers' hidden states at each token.\n",
    "    'avg_top2': average of the last two layers.\n",
    "    'avg_first_last': average of the first and the last layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, pooler_type):\n",
    "        super().__init__()\n",
    "        self.pooler_type = pooler_type\n",
    "        assert self.pooler_type in [\"cls\", \"cls_before_pooler\", \"avg\", \"avg_top2\", \"avg_first_last\"], \"unrecognized pooling type %s\" % self.pooler_type\n",
    "\n",
    "    def forward(self, attention_mask, outputs):\n",
    "        last_hidden = outputs.last_hidden_state\n",
    "        pooler_output = outputs.pooler_output\n",
    "        hidden_states = outputs.hidden_states\n",
    "\n",
    "        if self.pooler_type in ['cls_before_pooler', 'cls']:\n",
    "            return last_hidden[:, 0]\n",
    "        elif self.pooler_type == \"avg\":\n",
    "            return ((last_hidden * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(-1).unsqueeze(-1))\n",
    "        elif self.pooler_type == \"avg_first_last\":\n",
    "            first_hidden = hidden_states[0]\n",
    "            last_hidden = hidden_states[-1]\n",
    "            pooled_result = ((first_hidden + last_hidden) / 2.0 * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(-1).unsqueeze(-1)\n",
    "            return pooled_result\n",
    "        elif self.pooler_type == \"avg_top2\":\n",
    "            second_last_hidden = hidden_states[-2]\n",
    "            last_hidden = hidden_states[-1]\n",
    "            pooled_result = ((last_hidden + second_last_hidden) / 2.0 * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(-1).unsqueeze(-1)\n",
    "            return pooled_result\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "def cl_init(cls, config):\n",
    "    \"\"\"\n",
    "    Contrastive learning class init function.\n",
    "    \"\"\"\n",
    "    cls.pooler_type = cls.model_args.pooler_type\n",
    "    cls.pooler = Pooler(cls.model_args.pooler_type)\n",
    "    if cls.model_args.pooler_type == \"cls\":\n",
    "        cls.mlp = MLPLayer(config)\n",
    "    cls.sim = Similarity(temp=cls.model_args.temp)\n",
    "    cls.init_weights()\n",
    "\n",
    "def cl_forward(cls,\n",
    "    encoder,\n",
    "    input_ids=None,\n",
    "    attention_mask=None,\n",
    "    token_type_ids=None,\n",
    "    position_ids=None,\n",
    "    head_mask=None,\n",
    "    inputs_embeds=None,\n",
    "    labels=None,\n",
    "    output_attentions=None,\n",
    "    output_hidden_states=None,\n",
    "    return_dict=None,\n",
    "    past_key_values=None,\n",
    "    mlm_input_ids=None,\n",
    "    mlm_labels=None,\n",
    "):\n",
    "    return_dict = return_dict if return_dict is not None else cls.config.use_return_dict\n",
    "    ori_input_ids = input_ids\n",
    "    batch_size = input_ids.size(0)\n",
    "    # Number of sentences in one instance\n",
    "    # 2: pair instance; 3: pair instance with a hard negative\n",
    "    num_sent = input_ids.size(1)\n",
    "\n",
    "    mlm_outputs = None\n",
    "    # Flatten input for encoding\n",
    "    input_ids = input_ids.view((-1, input_ids.size(-1))) # (bs * num_sent, len)\n",
    "    attention_mask = attention_mask.view((-1, attention_mask.size(-1))) # (bs * num_sent len)\n",
    "    if token_type_ids is not None:\n",
    "        token_type_ids = token_type_ids.view((-1, token_type_ids.size(-1))) # (bs * num_sent, len)\n",
    "        \n",
    "    ##########################################################################\n",
    "    past_key_values = cls.get_prompt(batch_size=input_ids.shape[0])\n",
    "    prefix_attention_mask = torch.ones(input_ids.shape[0], cls.pre_seq_len).to(cls.device)\n",
    "    attention_mask = torch.cat((prefix_attention_mask, attention_mask), dim=1)\n",
    "    ##########################################################################\n",
    "\n",
    "    # Get raw embeddings\n",
    "    outputs = encoder(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        token_type_ids=token_type_ids,\n",
    "        position_ids=position_ids,\n",
    "        head_mask=head_mask,\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        output_attentions=output_attentions,\n",
    "        output_hidden_states=True if cls.model_args.pooler_type in ['avg_top2', 'avg_first_last'] else False,\n",
    "        return_dict=True,\n",
    "        past_key_values=past_key_values, # new added\n",
    "    )\n",
    "    \n",
    "    # MLM auxiliary objective\n",
    "    if mlm_input_ids is not None:\n",
    "        mlm_input_ids = mlm_input_ids.view((-1, mlm_input_ids.size(-1)))\n",
    "        mlm_outputs = encoder(\n",
    "            mlm_input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=True if cls.model_args.pooler_type in ['avg_top2', 'avg_first_last'] else False,\n",
    "            return_dict=True,\n",
    "            past_key_values=past_key_values, # new added\n",
    "        )\n",
    "\n",
    "    # Pooling\n",
    "    pooler_output = cls.pooler(attention_mask, outputs) # 这里attention mask需要修改\n",
    "    pooler_output = pooler_output.view((batch_size, num_sent, pooler_output.size(-1))) # (bs, num_sent, hidden)\n",
    "\n",
    "    # If using \"cls\", we add an extra MLP layer\n",
    "    # (same as BERT's original implementation) over the representation.\n",
    "    if cls.pooler_type == \"cls\":\n",
    "        pooler_output = cls.mlp(pooler_output)\n",
    "\n",
    "    # Separate representation\n",
    "    z1, z2 = pooler_output[:,0], pooler_output[:,1]\n",
    "\n",
    "    # Hard negative\n",
    "    if num_sent == 3:\n",
    "        z3 = pooler_output[:, 2]\n",
    "\n",
    "    # Gather all embeddings if using distributed training\n",
    "    if dist.is_initialized() and cls.training:\n",
    "        # Gather hard negative\n",
    "        if num_sent >= 3:\n",
    "            z3_list = [torch.zeros_like(z3) for _ in range(dist.get_world_size())]\n",
    "            dist.all_gather(tensor_list=z3_list, tensor=z3.contiguous())\n",
    "            z3_list[dist.get_rank()] = z3\n",
    "            z3 = torch.cat(z3_list, 0)\n",
    "\n",
    "        # Dummy vectors for allgather\n",
    "        z1_list = [torch.zeros_like(z1) for _ in range(dist.get_world_size())]\n",
    "        z2_list = [torch.zeros_like(z2) for _ in range(dist.get_world_size())]\n",
    "        # Allgather\n",
    "        dist.all_gather(tensor_list=z1_list, tensor=z1.contiguous())\n",
    "        dist.all_gather(tensor_list=z2_list, tensor=z2.contiguous())\n",
    "\n",
    "        # Since allgather results do not have gradients, we replace the\n",
    "        # current process's corresponding embeddings with original tensors\n",
    "        z1_list[dist.get_rank()] = z1\n",
    "        z2_list[dist.get_rank()] = z2\n",
    "        # Get full batch embeddings: (bs x N, hidden)\n",
    "        z1 = torch.cat(z1_list, 0)\n",
    "        z2 = torch.cat(z2_list, 0)\n",
    "\n",
    "    cos_sim = cls.sim(z1.unsqueeze(1), z2.unsqueeze(0))\n",
    "    # Hard negative\n",
    "    if num_sent >= 3:\n",
    "        z1_z3_cos = cls.sim(z1.unsqueeze(1), z3.unsqueeze(0))\n",
    "        cos_sim = torch.cat([cos_sim, z1_z3_cos], 1)\n",
    "\n",
    "    labels = torch.arange(cos_sim.size(0)).long().to(cls.device)\n",
    "    loss_fct = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Calculate loss with hard negatives\n",
    "    if num_sent == 3:\n",
    "        # Note that weights are actually logits of weights\n",
    "        z3_weight = cls.model_args.hard_negative_weight\n",
    "        weights = torch.tensor(\n",
    "            [[0.0] * (cos_sim.size(-1) - z1_z3_cos.size(-1)) + [0.0] * i + [z3_weight] + [0.0] * (z1_z3_cos.size(-1) - i - 1) for i in range(z1_z3_cos.size(-1))]\n",
    "        ).to(cls.device)\n",
    "        cos_sim = cos_sim + weights\n",
    "\n",
    "    loss = loss_fct(cos_sim, labels)\n",
    "\n",
    "    # Calculate loss for MLM\n",
    "    if mlm_outputs is not None and mlm_labels is not None:\n",
    "        mlm_labels = mlm_labels.view(-1, mlm_labels.size(-1))\n",
    "        prediction_scores = cls.lm_head(mlm_outputs.last_hidden_state)\n",
    "        masked_lm_loss = loss_fct(prediction_scores.view(-1, cls.config.vocab_size), mlm_labels.view(-1))\n",
    "        loss = (loss, cls.model_args.mlm_weight * masked_lm_loss)\n",
    "\n",
    "    if not return_dict:\n",
    "        output = (cos_sim,) + outputs[2:]\n",
    "        return ((loss,) + output) if loss is not None else output\n",
    "    return SequenceClassifierOutput(\n",
    "        loss=loss,\n",
    "        logits=cos_sim,\n",
    "        hidden_states=outputs.hidden_states,\n",
    "        attentions=outputs.attentions,\n",
    "    )\n",
    "\n",
    "\n",
    "def sentemb_forward(\n",
    "    cls,\n",
    "    encoder,\n",
    "    input_ids=None,\n",
    "    attention_mask=None,\n",
    "    token_type_ids=None,\n",
    "    position_ids=None,\n",
    "    head_mask=None,\n",
    "    inputs_embeds=None,\n",
    "    labels=None,\n",
    "    output_attentions=None,\n",
    "    output_hidden_states=None,\n",
    "    return_dict=None,\n",
    "):\n",
    "\n",
    "    return_dict = return_dict if return_dict is not None else cls.config.use_return_dict\n",
    "    \n",
    "    ##########################################################################\n",
    "    past_key_values = cls.get_prompt(batch_size=input_ids.shape[0])\n",
    "    prefix_attention_mask = torch.ones(input_ids.shape[0], cls.pre_seq_len).to(cls.device)\n",
    "    attention_mask = torch.cat((prefix_attention_mask, attention_mask), dim=1)\n",
    "    ##########################################################################\n",
    "    \n",
    "    outputs = encoder(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        token_type_ids=token_type_ids,\n",
    "        position_ids=position_ids,\n",
    "        head_mask=head_mask,\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        output_attentions=output_attentions,\n",
    "        output_hidden_states=True if cls.pooler_type in ['avg_top2', 'avg_first_last'] else False,\n",
    "        return_dict=True,\n",
    "        past_key_values=past_key_values, # new added\n",
    "    )\n",
    "\n",
    "    pooler_output = cls.pooler(attention_mask, outputs)\n",
    "    if cls.pooler_type == \"cls\" and not cls.model_args.mlp_only_train:\n",
    "        pooler_output = cls.mlp(pooler_output)\n",
    "\n",
    "    if not return_dict:\n",
    "        return (outputs[0], pooler_output) + outputs[2:]\n",
    "\n",
    "    return BaseModelOutputWithPoolingAndCrossAttentions(\n",
    "        pooler_output=pooler_output,\n",
    "        last_hidden_state=outputs.last_hidden_state,\n",
    "        hidden_states=outputs.hidden_states,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d981610f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobertaForCL(RobertaPreTrainedModel):\n",
    "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
    "\n",
    "    def __init__(self, config, *model_args, **model_kargs):\n",
    "        super().__init__(config)\n",
    "        self.model_args = model_kargs[\"model_args\"]\n",
    "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
    "        \n",
    "        ######################################################################\n",
    "        \n",
    "        self.pre_seq_len = self.model_args.pre_seq_len\n",
    "        self.n_layer = config.num_hidden_layers\n",
    "        self.n_head = config.num_attention_heads\n",
    "        self.n_embd = config.hidden_size // config.num_attention_heads\n",
    "        \n",
    "        self.prefix_tokens = torch.arange(self.pre_seq_len).long()\n",
    "        self.prefix_encoder = PrefixEncoder(config, self.model_args)\n",
    "        self.dropout = torch.nn.Dropout(config.hidden_dropout_prob)\n",
    "        \n",
    "        self.linear = nn.Linear(768, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        for param in self.roberta.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.prefix_encoder.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        # compute the number of total parameters and tunable parameters\n",
    "        total_param = sum(p.numel() for p in self.parameters())\n",
    "        trainable_param = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        print('total param is {}, trainable param is {}'.format(total_param, trainable_param))\n",
    "        ######################################################################\n",
    "        if self.model_args.do_mlm:\n",
    "            self.lm_head = RobertaLMHead(config)\n",
    "\n",
    "        cl_init(self, config)\n",
    "        \n",
    "    ##########################################################################\n",
    "    def get_prompt(self, batch_size):\n",
    "        prefix_tokens = self.prefix_tokens.unsqueeze(0).expand(batch_size, -1).to(self.roberta.device)\n",
    "        past_key_values = self.prefix_encoder(prefix_tokens)\n",
    "        # bsz, seqlen, _ = past_key_values.shape\n",
    "        past_key_values = past_key_values.view(\n",
    "            batch_size,\n",
    "            self.pre_seq_len,\n",
    "            self.n_layer * 2, \n",
    "            self.n_head,\n",
    "            self.n_embd\n",
    "        )\n",
    "        past_key_values = self.dropout(past_key_values)\n",
    "        past_key_values = past_key_values.permute([2, 0, 3, 1, 4]).split(2)\n",
    "        return past_key_values\n",
    "    ##########################################################################\n",
    "\n",
    "    def forward(self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        sent_emb=True,\n",
    "        mlm_input_ids=None,\n",
    "        mlm_labels=None,\n",
    "    ):\n",
    "        if sent_emb:\n",
    "            output = sentemb_forward(self, self.roberta,\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids,\n",
    "                position_ids=position_ids,\n",
    "                head_mask=head_mask,\n",
    "                inputs_embeds=inputs_embeds,\n",
    "                labels=labels,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                return_dict=return_dict,\n",
    "            ).pooler_output\n",
    "            \n",
    "            output = self.linear(output)\n",
    "            output = self.sigmoid(output)\n",
    "            return output\n",
    "            \n",
    "        else:\n",
    "            return cl_forward(self, self.roberta,\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids,\n",
    "                position_ids=position_ids,\n",
    "                head_mask=head_mask,\n",
    "                inputs_embeds=inputs_embeds,\n",
    "                labels=labels,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                return_dict=return_dict,\n",
    "                mlm_input_ids=mlm_input_ids,\n",
    "                mlm_labels=mlm_labels,\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8dc52b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_custom(Dataset): \n",
    "\tdef __init__(self, path):\n",
    "\t\twith open(path, 'r') as f:\n",
    "\t\t\tself.x = json.load(f)\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.x)\n",
    "    \n",
    "\tdef __getitem__(self, idx):\n",
    "\t\treturn self.x[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6dfa4832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "\n",
    "    input_sentences = [sample[1] for sample in batch]\n",
    "    input_labels = [sample[2] for sample in batch]\n",
    "\n",
    "    batch_inputs = tokenizer(input_sentences, padding = True, return_tensors = \"pt\")\n",
    "    batch_labels = make_batch(input_labels)\n",
    "\n",
    "    return batch_inputs, batch_labels\n",
    "\n",
    "def make_batch(labels):\n",
    "    batch_labels = []\n",
    "    \n",
    "    for label in labels:\n",
    "        sample_label = [label]\n",
    "        batch_labels.append(sample_label)\n",
    "        \n",
    "    return torch.tensor(batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f171b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = Dataset_custom(PATH_train)\n",
    "dataset_test = Dataset_custom(PATH_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "239cc9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150000 50000\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "\t\tlen(dataset_train),\n",
    "\t\tlen(dataset_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c24e266",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "dataloader_train = DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=custom_collate_fn\n",
    ")\n",
    "\n",
    "dataloader_test = DataLoader(\n",
    "    dataset_test,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=custom_collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7707dc57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'klue/roberta-base'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_args = ModelArguments()\n",
    "model_args.model_name_or_path = PRETAINED_MODEL_NAME\n",
    "model_args.model_name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c89c937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total param is 110212609, trainable param is 185089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-base were not used when initializing RobertaForCL: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing RobertaForCL from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForCL from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForCL were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['mlp.dense.weight', 'prefix_encoder.embedding.weight', 'linear.bias', 'linear.weight', 'mlp.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = RobertaForCL.from_pretrained(\n",
    "                model_args.model_name_or_path,\n",
    "                from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
    "                config=config,\n",
    "                cache_dir=model_args.cache_dir,\n",
    "                revision=model_args.model_revision,\n",
    "                use_auth_token=True if model_args.use_auth_token else None,\n",
    "                model_args=model_args                  \n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "657bec23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "model.to(device)\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51acfcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "CELoss = nn.BCELoss()\n",
    "optimizer = AdamW(model.parameters(), lr=1.0e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c4c8183",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0fb9f7e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 421/2344 [03:48<17:24,  1.84it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-b6b628370d16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mTIME\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start=time.time()\n",
    "    train_loss = 0\n",
    "\n",
    "    for batch in tqdm(dataloader_train):\n",
    "        batch_inputs = {k: v.cuda(device) for k, v in list(batch[0].items())}\n",
    "        batch_labels = batch[1].cuda(device)\n",
    "\n",
    "        output = model(**batch_inputs)\n",
    "\n",
    "        loss = CELoss(output.view(-1).to(torch.float32), batch_labels.view(-1).to(torch.float32))\n",
    "        # loss = CELoss(output.view(-1, output.size(-1)), batch_labels.view(-1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()/len(dataloader_train)\n",
    "        \n",
    "    TIME = time.time() - start\n",
    "    print(f'epoch : {epoch+1}/{epochs}    time : {TIME:.0f}s    ETA : {TIME*(epochs-epoch-1):.0f}s')\n",
    "    print(f'TRAIN    loss : {train_loss:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addbaadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '../model/model_state_dict.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba256db6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 15/782 [00:03<03:05,  4.13it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-8548edbc8cbb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mbatch_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kcc/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-32f79387f0f5>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, sent_emb, mlm_input_ids, mlm_labels)\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             ).pooler_output\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-63dfff1af3ab>\u001b[0m in \u001b[0;36msentemb_forward\u001b[0;34m(cls, encoder, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpooler_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'avg_top2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'avg_first_last'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mpast_key_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# new added\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m     )\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kcc/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kcc/lib/python3.6/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    843\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 845\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    846\u001b[0m         )\n\u001b[1;32m    847\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kcc/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kcc/lib/python3.6/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    527\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m                     \u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m                 )\n\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kcc/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kcc/lib/python3.6/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    412\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself_attn_past_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         )\n\u001b[1;32m    416\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kcc/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kcc/lib/python3.6/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    342\u001b[0m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m         )\n\u001b[1;32m    346\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kcc/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kcc/lib/python3.6/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    271\u001b[0m             \u001b[0mattention_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_probs\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m         \u001b[0mcontext_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0mcontext_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "gold_list = []\n",
    "pred_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dataloader_test):\n",
    "        batch_inputs = {k: v.cuda(device) for k, v in list(batch[0].items())}\n",
    "        batch_labels = batch[1].cuda(device)\n",
    "        \n",
    "        output = model(**batch_inputs)\n",
    "        loss = CELoss(output.view(-1).to(torch.float32), batch_labels.view(-1).to(torch.float32))\n",
    "        \n",
    "        # print('loss:', loss.item())\n",
    "        \n",
    "        for gold, pred in zip(batch_labels, output):\n",
    "            pred = torch.round(pred)\n",
    "            # pred = pred.to(torch.int32)\n",
    "\n",
    "            gold_list.append(gold)\n",
    "            pred_list.append(pred)\n",
    "            \n",
    "            # print(gold)\n",
    "            # print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26ff17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_list_flat = []\n",
    "pred_list_flat = []\n",
    "for g, p in zip(gold_list, pred_list):\n",
    "    gold_list_flat.append(g.item())\n",
    "    pred_list_flat.append(p.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244e2fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.899443  0.890965  0.895184     24827\n",
      "           1   0.893455  0.901760  0.897588     25173\n",
      "\n",
      "    accuracy                       0.896400     50000\n",
      "   macro avg   0.896449  0.896363  0.896386     50000\n",
      "weighted avg   0.896428  0.896400  0.896394     50000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(gold_list_flat, pred_list_flat, digits = 4)\n",
    "print(report)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
